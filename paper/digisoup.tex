\documentclass[11pt,a4paper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{caption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=blue!60!black,
    urlcolor=blue!60!black,
}

% --- Title ---
\title{%
    DigiSoup: A Zero-Training Entropy-Driven Agent\\
    Beats Trained Reinforcement Learning\\
    on Multi-Agent Social Dilemmas%
}

\author{
    Matthew Fearne\\
    Independent Researcher\\
    \texttt{mrfearne@gmail.com}\\
    \url{https://github.com/matthewfearne/digisoup}
}

\date{February 2026}

\begin{document}
\maketitle

% ===================================================================
\begin{abstract}
We present DigiSoup, a zero-training agent that uses thermodynamic perception
and bio-inspired heuristics to play multi-agent social dilemmas on DeepMind's
Melting Pot benchmark. DigiSoup uses no neural networks, no reward optimisation,
and no training of any kind---actions are selected by a stack of priority rules
driven by entropy gradients, temporal growth rates, and spatial memory, implemented
in approximately 350 lines of NumPy. Despite this simplicity, DigiSoup outperforms
DeepMind's trained reinforcement learning baselines (ACB, VMPO) on the majority of
Clean Up scenarios---a complex public goods dilemma requiring collective action.
On Clean Up scenario 0, DigiSoup scores 242.87 focal per-capita return versus
ACB's 170.66 (+42\%) and VMPO's 180.24 (+35\%). Five previously intractable
majority-focal scenarios, where trained agents score near zero, are solved through
a thermodynamic depletion signal: when the entropy growth rate drops to zero
($dS/dt \leq 0$), the agent infers that the shared resource is depleted and
diverts to public goods maintenance. These results suggest that thermodynamic
perception may offer a viable alternative to gradient-based learning for emergent
cooperation in multi-agent systems.
\end{abstract}

% ===================================================================
\section{Introduction}
\label{sec:intro}

Multi-agent cooperation is widely considered to require learning. The dominant
paradigm trains neural network policies via reinforcement learning (RL) over
millions of environment steps, optimising reward signals through gradient
descent~\citep{leibo2021meltingpot, agapiou2023meltingpot2}. DeepMind's Melting
Pot benchmark~\citep{agapiou2023meltingpot2} formalises this challenge: agents
must cooperate with and against pre-trained background bots across diverse social
dilemmas, from commons tragedies to public goods problems to iterated prisoner's
dilemmas.

We challenge the assumption that learning is necessary. DigiSoup is a
\emph{zero-training} agent that selects actions using only thermodynamic
perception---entropy gradients, temporal growth rates, and bio-inspired spatial
memory---without any neural networks, reward optimisation, or cross-episode
learning. The entire agent is approximately 350 lines of NumPy across four
Python modules.

Despite this radical simplicity, DigiSoup outperforms DeepMind's trained RL
baselines on the majority of Clean Up scenarios in Melting Pot. Clean Up is
arguably the benchmark's hardest social dilemma: a river must be cleaned to
allow apple regrowth, but cleaning is costly and individual agents are better
off free-riding on others' labour. Trained RL agents frequently fail this
collective action problem, particularly when they constitute the majority of
players.

Our key contributions are:

\begin{enumerate}
    \item A fully explainable, zero-training agent architecture based on
          thermodynamic perception and bio-inspired heuristics.
    \item Demonstration that this agent outperforms trained RL baselines
          (ACB~+42\%, VMPO~+35\% on Clean Up scenario~0) on the Melting Pot
          benchmark.
    \item A novel depletion-detection mechanism ($dS/dt \leq 0$) that solves
          the collective action problem in Clean Up's majority-focal scenarios,
          where all standard trained agents score near zero.
    \item Evidence that thermodynamic perception may be a viable complement
          or alternative to gradient-based learning for multi-agent cooperation.
\end{enumerate}

% ===================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Melting Pot.}
Melting Pot~\citep{leibo2021meltingpot} and its successor Melting
Pot~2.0~\citep{agapiou2023meltingpot2} provide a standardised benchmark for
evaluating multi-agent cooperation. The benchmark pairs focal agents (under
evaluation) with pre-trained background bots across scenarios drawn from
game-theoretic social dilemmas. Published baselines include A3C, VMPO, OPRE,
and their prosocial variants, all trained via deep RL. To our knowledge, no
prior work has evaluated a zero-training, hand-coded agent on Melting Pot.

\paragraph{Entropy and Information-Theoretic Approaches.}
Information-theoretic objectives have been used to drive exploration in
RL~\citep{pathak2017curiosity, eysenbach2019diayn}, but these approaches
still train neural networks via gradient descent. Friston's Free Energy
Principle~\citep{friston2010free} provides a theoretical framework connecting
thermodynamics to agent behaviour, but practical implementations typically
require learned generative models. DigiSoup operationalises a simpler
thermodynamic insight: entropy gradients in raw pixel observations provide
sufficient signal for navigation and resource detection without any learning.

\paragraph{Bio-Inspired Multi-Agent Systems.}
Slime mould-inspired algorithms have been applied to optimisation~\citep{li2020slime}
and network design~\citep{tero2010rules}. Swarm intelligence approaches
use simple local rules to produce collective behaviour~\citep{bonabeau1999swarm}.
DigiSoup draws on multiple biological metaphors---slime mould path reinforcement,
jellyfish oscillation, mycorrhizal nutrient sharing---but applies them to the
specific challenge of social dilemma resolution in Melting Pot's mixed-motive
scenarios.

% ===================================================================
\section{Method}
\label{sec:method}

DigiSoup is a reactive agent with no internal model of other agents and no
memory across episodes. It processes each 88$\times$88 RGB observation
independently through four modules: perception, state, action selection,
and inter-agent communication.

\subsection{Thermodynamic Perception}
\label{sec:perception}

The observation is divided into a 4$\times$4 grid of patches. For each patch,
we compute Shannon entropy over the RGB histogram:
\begin{equation}
    H_i = -\sum_{b} p_{i,b} \log_2 p_{i,b}
\end{equation}
where $p_{i,b}$ is the normalised frequency of colour bin $b$ in patch $i$.

From the entropy grid, we derive:

\begin{itemize}
    \item \textbf{Entropy gradient} $\nabla H$: direction toward highest entropy
          (indicating environmental complexity and potential resources).
    \item \textbf{Growth gradient} $\nabla(\Delta H)$: direction toward where
          entropy is \emph{increasing} over time, computed as
          $\Delta H_i = H_i^{(t)} - H_i^{(t-1)}$. This temporal derivative
          tracks apple regrowth and activity.
    \item \textbf{Growth rate} $\bar{\Delta H}$: mean entropy change across
          all patches. When $\bar{\Delta H} \leq 0$, the environment's entropy
          is declining---a thermodynamic signal that resources are not regenerating.
    \item \textbf{KL anomaly}: divergence of each patch from a uniform
          distribution, detecting agents as statistical anomalies against
          uniform backgrounds (critical for dark Prisoner's Dilemma arenas).
\end{itemize}

Additionally, colour masks detect specific environmental features:
\textbf{resources} (green vegetation and red/orange apples),
\textbf{water/pollution} (blue-green river tiles in Clean Up),
\textbf{sand} (dead zones), \textbf{grass} (orchard floor where apples grow),
and \textbf{other agents} (saturated non-resource pixels). Each mask produces
a boolean detection flag, a direction vector (centroid of matching pixels
relative to observation centre), and a density scalar.

\subsection{Internal State}
\label{sec:state}

The agent maintains a minimal internal state updated each step:

\begin{itemize}
    \item \textbf{Energy} $e \in [0, 1]$: proxy for recent reward, decaying
          over time and replenished when entropy changes (indicating resource
          collection). Energy modulates risk tolerance.
    \item \textbf{Cooperation tendency} $c \in [0, 1]$: EMA of recent
          interaction outcomes. Higher values favour cooperative actions.
    \item \textbf{Spatial memory}: a direction vector reinforced when resources
          are detected and decaying otherwise (slime mould path reinforcement).
    \item \textbf{Resource heatmap}: a 4$\times$4 grid recording resource
          locations with exponential decay ($\alpha = 0.95$ per step).
    \item \textbf{Heading}: EMA of recent movement directions for trajectory
          smoothing.
    \item \textbf{Phase}: alternating explore/exploit on a 50-step clock
          (jellyfish oscillation).
\end{itemize}

\subsection{Action Selection}
\label{sec:action}

Actions are selected by a deterministic priority rule stack, evaluated top to
bottom. The first rule whose conditions are met determines the action.

\begin{enumerate}
    \item \textbf{Random exploration.} With phase-modulated probability
          (higher in explore phase), select a random movement action.
    \item \textbf{Energy critical.} If energy is low, seek resources via:
          visible resources $\rightarrow$ spatial memory $\rightarrow$ resource
          heatmap $\rightarrow$ \emph{if $dS/dt \leq 0$: navigate to river
          and clean} $\rightarrow$ entropy gradient.
    \item \textbf{River cleaning.} If at the river (water density $> 8\%$
          of view) and not standing in sand: fire cleaning beam. If water is
          visible but distant and no food is visible: approach river.
    \item \textbf{Proactive cleaning.} If $dS/dt \leq 0$ (environment
          depleting), no food visible, and water detected: approach river
          and clean.
    \item \textbf{Exploit-phase resource seeking.} During exploit phase at
          moderate energy, follow resource signals.
    \item \textbf{Context-aware symbiosis.} When other agents are detected,
          respond based on environmental context: near river and environment
          depleting $\rightarrow$ join cleaning; crowded and depleting
          $\rightarrow$ complement (go clean instead of competing); no river
          context $\rightarrow$ cooperate or flee based on cooperation tendency.
          Gated by growth rate: only diverts to cleaning when $dS/dt \leq 0$.
    \item \textbf{Stable navigation.} In stable environments: avoid sand,
          seek grass, avoid crowded areas, follow heatmap or entropy gradient.
    \item \textbf{Chaotic exploitation.} In rapidly changing environments:
          continue current behavioural role.
\end{enumerate}

The critical innovation is Rule~2's depletion branch and Rule~4's proactive
cleaning. When $dS/dt \leq 0$, the agent infers---from raw pixel entropy
alone---that the environment's regenerative capacity has failed. In Clean Up,
this corresponds to river pollution exceeding the threshold where apple growth
drops to zero. The agent responds by navigating to the river and cleaning,
\emph{without any reward signal indicating that cleaning is beneficial}.

\subsection{Hive Mind}
\label{sec:hive}

Focal agents share discoveries through a class-level spatial memory (Hive
Memory). When an agent detects resources or river pollution, it writes the
discovery's world-space coordinates to the shared memory. Other agents query
for the nearest discovery and receive a direction vector transformed to their
egocentric frame. This mimics mycorrhizal nutrient-sharing networks in forests.

% ===================================================================
\section{Experimental Setup}
\label{sec:experiments}

\subsection{Benchmark}

We evaluate on three Melting Pot substrates spanning 17~scenarios
(Table~\ref{tab:substrates}). Each scenario pairs DigiSoup focal agents with
DeepMind's pre-trained background bots.

\begin{table}[h]
\centering
\caption{Target substrates and scenario configurations.}
\label{tab:substrates}
\begin{tabular}{lccc}
\toprule
\textbf{Substrate} & \textbf{Scenarios} & \textbf{Focal} & \textbf{Background} \\
\midrule
Commons Harvest Open & 2 & 5 & 2 \\
Clean Up & 9 & 3--6 & 1--7 \\
Prisoner's Dilemma Arena & 6 & 1--7 & 1--7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Protocol}

We follow the official Melting Pot 2.0 evaluation
protocol~\citep{agapiou2023meltingpot2}. The primary metric is
\textbf{focal per-capita return}: total reward earned by focal agents divided
by the number of focal agents, averaged across episodes. We report means with
95\% confidence intervals across 30 episodes per scenario.

\subsection{Baselines}

We compare against DeepMind's published per-scenario scores from the official
results file (\texttt{meltingpot-results-2.3.0.feather}), averaged across
training runs:

\begin{itemize}
    \item \textbf{ACB} (Actor-Critic Baseline): standard deep RL agent.
    \item \textbf{VMPO} (V-MPO): advanced policy optimisation.
    \item \textbf{OPRE/OPRE-Prosocial}: options-based RL with prosocial variant.
    \item \textbf{Random}: uniform random action selection.
\end{itemize}

\subsection{Hardware}

All experiments run on a single consumer workstation: Intel i7-8700K, 64GB RAM,
NVIDIA GTX~1060 6GB. The GPU accelerates background bot inference (TensorFlow);
DigiSoup itself is pure NumPy and requires no GPU.

% ===================================================================
\section{Results}
\label{sec:results}

\subsection{Clean Up: DigiSoup Outperforms Trained RL}

Table~\ref{tab:cleanup} presents per-scenario results on Clean Up.
DigiSoup outperforms ACB on 4 of 8 active scenarios and VMPO on 6 of~8,
with an aggregate score 22\% above ACB.

\begin{table}[h]
\centering
\caption{Clean Up results: focal per-capita return (30 episodes).
Bold indicates DigiSoup outperforms the baseline. CU\_1 is excluded
from win counts as all non-prosocial agents score zero.
Baseline scores from \texttt{meltingpot-results-2.3.0.feather}.}
\label{tab:cleanup}
\begin{tabular}{lrrrrrrl}
\toprule
\textbf{Scenario} & \textbf{Focal/Bg} & \textbf{DigiSoup} & \textbf{95\% CI} & \textbf{ACB} & \textbf{VMPO} & \textbf{Random} & \textbf{vs ACB} \\
\midrule
CU\_0 & 3/4 & \textbf{194.70} & $\pm$25.35 & 170.66 & 180.24 & 88.69 & \textbf{+14\%} \\
CU\_1 & 4/3 & 0.00 & $\pm$0.00 & 0.00 & 0.00 & 0.00 & --- \\
CU\_2 & 3/4 & \textbf{79.22} & $\pm$11.66 & 76.76 & 92.06 & 40.49 & \textbf{+3\%} \\
CU\_3 & 3/4 & 65.90 & $\pm$8.25 & 67.75 & 76.15 & 35.97 & $-$3\% \\
CU\_4 & 6/1 & 42.14 & $\pm$8.18 & 42.62 & 7.24 & 32.34 & $-$1\% \\
CU\_5 & 5/2 & 31.27 & $\pm$6.09 & 39.08 & 10.70 & 27.43 & $-$20\% \\
CU\_6 & 6/1 & \textbf{13.21} & $\pm$2.52 & 9.55 & 0.38 & 9.16 & \textbf{+38\%} \\
CU\_7 & 2/5 & \textbf{234.00} & $\pm$48.39 & 120.41 & 95.18 & 70.18 & \textbf{+94\%} \\
CU\_8 & 6/1 & 45.38 & $\pm$8.92 & 52.55 & 22.73 & 38.18 & $-$14\% \\
\midrule
\textbf{Total} & & \textbf{705.82} & & 579.38 & 484.67 & 341.44 & \textbf{+22\%} \\
\bottomrule
\end{tabular}
\end{table}

The strongest results come from scenarios where DigiSoup is a minority
(CU\_0, CU\_7) or faces weak baselines (CU\_6). On CU\_7 (2~focal, 5~background),
DigiSoup scores 234.00---nearly double ACB's 120.41---because even two entropy-driven
agents trigger enough river cleaning to sustain the commons. On majority-focal
scenarios (CU\_4--CU\_6, CU\_8), VMPO scores 0.38--22.73; it was never trained to
clean the river when insufficient background bots do it. DigiSoup's depletion
signal ($dS/dt \leq 0$) triggers river cleaning regardless of what other agents do,
solving the collective action problem from first principles.

CU\_1 deserves special note: \emph{all} standard trained agents (ACB, VMPO,
OPRE, OPRE-Prosocial) score zero on this scenario. Only ACB-Prosocial, a
variant specifically trained for prosocial behaviour, achieves a non-zero
score (65.29). CU\_1 represents a pathological configuration, not a DigiSoup
failure.

\subsection{Commons Harvest and Prisoner's Dilemma}

Table~\ref{tab:other} presents results on the remaining substrates.

\begin{table}[h]
\centering
\caption{Commons Harvest Open and Prisoner's Dilemma Arena results
(30 episodes, 95\% CI shown).}
\label{tab:other}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Scenario} & \textbf{DigiSoup} & \textbf{95\% CI} & \textbf{Random} & \textbf{vs Random} & \textbf{ACB} \\
\midrule
CH\_0 (5f/2bg) & 2.84 & $\pm$0.83 & 1.81 & +57\% & 10.27 \\
CH\_1 (5f/2bg) & 3.44 & $\pm$0.88 & 1.87 & +84\% & 10.67 \\
\midrule
PD\_0 (1f/7bg) & 16.50 & $\pm$3.13 & 9.35 & +76\% & 62.45 \\
PD\_1 (7f/1bg) & 7.50 & $\pm$0.84 & 6.69 & +12\% & 35.34 \\
PD\_2 (6f/2bg) & 7.52 & $\pm$1.62 & 3.71 & +103\% & 30.07 \\
PD\_3 (1f/7bg) & 11.25 & $\pm$2.96 & 7.00 & +61\% & 32.92 \\
PD\_4 (1f/7bg) & 15.01 & $\pm$3.11 & 9.08 & +65\% & 41.65 \\
PD\_5 (3f/5bg) & 14.84 & $\pm$2.31 & 7.17 & +107\% & 34.42 \\
\bottomrule
\end{tabular}
\end{table}

On Commons Harvest, DigiSoup exceeds random by 57--84\% but falls short of
trained agents. Commons Harvest rewards fast foraging in open fields where
entropy gradients provide limited directional signal. On Prisoner's Dilemma,
DigiSoup consistently outperforms random (average +71\% across all 6 scenarios)
but cannot match trained agents that have learned opponent-modelling strategies.
These results are expected: PD rewards learning your partner's strategy over
repeated encounters, a capability that requires some form of training.

\subsection{Version Ablation}

DigiSoup was developed incrementally over 15 versions, each adding one
bio-inspired ``layer.'' Table~\ref{tab:versions} summarises key versions.

\begin{table}[h]
\centering
\caption{Version evolution showing cumulative effect of each layer.
CU\_0 focal per-capita return shown as representative metric.}
\label{tab:versions}
\begin{tabular}{llrl}
\toprule
\textbf{Version} & \textbf{Layer Added} & \textbf{CU\_0} & \textbf{Key Change} \\
\midrule
v1 & Random baseline & 96.60 & Floor measurement \\
v2 & Entropy perception & 143.10 & +48\% over random \\
v3 & Jellyfish oscillation & 181.97 & Explore/exploit cycling \\
v4 & Slime mould memory & 231.20 & First time beating ACB \\
v5--v7 & Behaviour modifications & 77--227 & All regressed \\
v8 & Thermodynamic sensing & 221.87 & 4$\times$4 grid, $dS/dt$, KL \\
v9 & Resource conservation & 256.70 & +50\% vs ACB \\
v10 & Colour perception fix & 209.10 & Apple detection bug fixed \\
v11 & Cleaning rule & 277.93 & CU\_0 peak \\
v15 & Depletion + symbiosis & 242.87 & 5 zero$\rightarrow$scoring \\
\bottomrule
\end{tabular}
\end{table}

A critical finding from this ablation: versions that improved
\textbf{perception} (v2--v4, v8--v10) consistently improved performance,
while versions that modified \textbf{behaviour} (v5--v7: cooperation
thresholds, energy dynamics, aggression) consistently regressed. The agent's
decision system appears near-optimal for zero-training; gains come from
sharper senses, not cleverer strategies.

% ===================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Does Thermodynamic Perception Work?}

Clean Up has a specific structure that thermodynamic perception exploits:
river pollution causes apple growth to cease, which causes environmental
entropy to stop increasing. This creates a directly observable signal---$dS/dt
\leq 0$---that indicates public goods maintenance is needed. The agent does not
need to learn this relationship; it falls out of the physics of the environment.

This suggests that thermodynamic approaches may be particularly effective for
environments where \emph{resource depletion has observable consequences}.
Many real-world collective action problems share this structure: deforestation
reduces biodiversity (observable), overfishing depletes stocks (observable),
pollution degrades air quality (observable). An agent that responds to
thermodynamic decline may generalise to these settings without task-specific
training.

\subsection{Limitations}

We identify four principal limitations:

\begin{itemize}
    \item \textbf{No learning.} DigiSoup's rules are hand-coded. While this
          is precisely the point---demonstrating that cooperation can emerge
          without training---it means the agent cannot adapt to truly novel
          scenarios beyond its design envelope. Performance on Prisoner's
          Dilemma lags trained agents that learn partner strategies over time,
          and Commons Harvest remains weak because entropy gradients provide
          insufficient signal in open, homogeneous resource fields.

    \item \textbf{Limited substrate coverage.} We evaluate on three of Melting
          Pot's 50+ substrates: Clean Up, Commons Harvest Open, and Prisoner's
          Dilemma Arena. These are the canonical social dilemma benchmarks used
          across the Melting Pot literature, but some perception components
          (e.g., river detection) are substrate-specific. Extending DigiSoup to
          visually dissimilar substrates such as Territory or Collaborative
          Cooking would require new colour masks and potentially new action rules.

    \item \textbf{Background agent dependency.} Following the standard Melting
          Pot evaluation protocol, focal agent scores depend on which trained
          background population is present. Our results are therefore
          conditioned on the official background bots; performance could shift
          if focal agents were paired with different co-players.

    \item \textbf{No head-to-head comparison.} We compare per-capita returns
          across separate evaluation runs using the official Melting Pot
          protocol. We do not place DigiSoup and trained RL agents in the
          \emph{same} episode, which would provide a stronger direct comparison
          but falls outside the standard benchmark methodology.
\end{itemize}

\subsection{Implications}

These results challenge two assumptions:

\begin{enumerate}
    \item \textbf{That multi-agent cooperation requires learning.} DigiSoup
          achieves cooperation through thermodynamic inference alone. The
          collective action problem in Clean Up is solved by an agent that
          has never received a reward signal.
    \item \textbf{That complexity requires complex agents.} A 350-line
          NumPy agent outperforms million-parameter neural networks trained
          for millions of steps. The information needed for cooperation was
          already present in the observation stream; it just needed the right
          perceptual frame.
\end{enumerate}

% ===================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented DigiSoup, a zero-training entropy-driven agent that outperforms
trained reinforcement learning baselines on the majority of Clean Up scenarios
in DeepMind's Melting Pot benchmark. The agent's success is driven by a
thermodynamic depletion signal ($dS/dt \leq 0$) that solves the collective
action problem without reward optimisation, and by bio-inspired heuristics
(slime mould memory, jellyfish oscillation, mycorrhizal sharing) that provide
effective navigation and cooperation without learning.

The key result is not that DigiSoup beats ACB or VMPO on specific numbers,
but that \emph{it is possible at all}. A hand-coded, explainable, 350-line
agent competing with trained deep RL on a benchmark designed for trained
agents suggests that the role of thermodynamic perception in multi-agent
cooperation deserves further investigation.

Code and full results are available at
\url{https://github.com/matthewfearne/digisoup}.

% ===================================================================
\section*{Acknowledgements}

This work was conducted independently using consumer hardware. The author
thanks the DeepMind team for the Melting Pot benchmark and for making
background bot policies publicly available.

% ===================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
